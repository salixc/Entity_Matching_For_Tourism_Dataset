{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080 259\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers.util import cos_sim  \n",
    "from sentence_transformers import SentenceTransformer as SBert\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy import sqrt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import os\n",
    "# modelName = 'princeton-nlp/sup-simcse-bert-base-uncased'\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "dataName1 = \"Ctrip\"\n",
    "dataName2 = \"Mafengwo\"\n",
    "\n",
    "with open(\"../train_data/%s_addr.json\"%dataName1, \"r\", encoding=\"utf-8\") as f:\n",
    "    data1 = json.load(f)\n",
    "with open(\"../train_data/%s_addr.json\"%dataName2, \"r\", encoding=\"utf-8\") as f:\n",
    "    data2 = json.load(f)\n",
    "print(len(data1), len(data2))\n",
    "\n",
    "positive, negative = [], []\n",
    "matched = set()\n",
    "# ! Attention !\n",
    "# Since this 'matched' set runs through the entire file to prevent the extraction of \n",
    "# duplicate entity pairs, you cannot run a single block of code repeatedly.\n",
    "# If you want to repeatedly run a certain code block,\n",
    "# you need to run it from the first code block (which is where it is now) one by one\n",
    "# to ensure the correctness of the matched set.\n",
    "\n",
    "def combineEnt(d1, d2, label):\n",
    "    rnt = {\"id\": \"%s-%s\"%(d1[\"id\"], d2[\"id\"])}\n",
    "    rnt[\"label\"] = label\n",
    "    for attr in list(d1.keys())[1:]:\n",
    "        rnt[attr] = [d1[attr], d2[attr]]\n",
    "    return rnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [00:33<00:00, 40.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Do word vector calculations on all occurrences of strings in advance \n",
    "# to reduce the time complexity of subsequent processes\n",
    "if os.path.exists(\"temp/pre_dict.json\"):\n",
    "    print(\"Preprocessing dictionary file already exists!\")\n",
    "else:\n",
    "    eDict = {}\n",
    "    cols = [\"Name\", \"Address\", \"District\", \"RoadInfo\", \"Poi\", \"RoomInfo\"]\n",
    "    data = data1 + data2\n",
    "    for i in trange(len(data)):\n",
    "        d = data[i]\n",
    "        for col in cols:\n",
    "            target = d[col]\n",
    "            if target == None or target == \"\": continue\n",
    "            if target not in eDict:\n",
    "                eDict[target] = model.encode(target).tolist()\n",
    "    with open(\"./temp/pre_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eDict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name - Generate similar top 200\n",
    "\n",
    "if os.path.exists(\"temp/name.csv\"):\n",
    "    print(\"Top 200 file (Name) already exist!\")\n",
    "else:\n",
    "    with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        eDict = json.load(f)\n",
    "    lst_name = []\n",
    "    for i in trange(len(data1)):\n",
    "        d1 = data1[i]\n",
    "        for d2 in data2:\n",
    "            name1 = d1[\"Name\"]; name2 = d2[\"Name\"]\n",
    "            if name1 == \"\" or name2 == \"\": continue\n",
    "            cossim = 1 - cosine(eDict[name1], eDict[name2])\n",
    "            lst_name.append([d1[\"id\"]+\"-\"+d2[\"id\"], name1, name2, cossim])\n",
    "    lst_name = sorted(lst_name, key=lambda x: x[3], reverse=True)\n",
    "    lst_name = lst_name[:200]\n",
    "    df_name = pd.DataFrame(lst_name, columns=[\"id\", \"Name1\", \"Name2\", \"Cos_sim_name\"])\n",
    "    df_name.to_csv(\"./temp/name.csv\", encoding=\"utf-8\")\n",
    "    # Next, in the CSV file generated above, create a new label column with 1/0 representing match/mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name - label and extract the reuslt\n",
    "\n",
    "with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eDict = json.load(f)\n",
    "df_name = pd.read_csv(\"./temp/name.csv\", encoding=\"utf-8\")\n",
    "namePos, nameNeg = [], []\n",
    "if \"label\" not in df_name.columns:\n",
    "    print(\"Please label the CSV file generated above first!\")\n",
    "else:\n",
    "    for i in range(len(df_name)):\n",
    "        id = df_name.loc[i, \"id\"].split(\"-\")\n",
    "        id1, id2 = int(id[0][1:]), int(id[1][1:])\n",
    "        d1, d2 = data1[id1], data2[id2]\n",
    "        if d1[\"Name\"] != df_name.loc[i, \"Name1\"] or d2[\"Name\"] != df_name.loc[i, \"Name2\"]:\n",
    "            print(\"There was an error in the process of responding to the dataset! Please check\")\n",
    "        matched.add(d1[\"id\"]+d2[\"id\"])\n",
    "        if df_name.loc[i, \"label\"] == 1:\n",
    "            namePos.append(combineEnt(d1, d2, True))\n",
    "        else:\n",
    "            nameNeg.append(combineEnt(d1, d2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "距离Top200文件已经存在！\n"
     ]
    }
   ],
   "source": [
    "# Distance - Generate similar top 200\n",
    "\n",
    "if os.path.exists(\"temp/distance.csv\"):\n",
    "    print(\"Top 200 file (Distance) already exist!\")\n",
    "else:\n",
    "    from geopy.distance import geodesic\n",
    "    with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        eDict = json.load(f)\n",
    "    lst_distance = []\n",
    "    for i in trange(len(data1)):\n",
    "        d1 = data1[i]\n",
    "        for d2 in data2:\n",
    "            name1 = d1[\"Name\"]; name2 = d2[\"Name\"]\n",
    "            loc1 = d1[\"Location\"]; loc2 = d2[\"Location\"]\n",
    "            if loc1[0] is None or loc2[0] is None: continue\n",
    "            if d1[\"id\"] + d2[\"id\"] in matched: continue\n",
    "            distance = geodesic((loc1[1], loc1[0]), (loc2[1], loc2[0])).meters\n",
    "            if distance == 0: continue  # Identical coordinates may be incorrect values\n",
    "            lst_distance.append([d1[\"id\"]+\"-\"+d2[\"id\"], name1, name2, distance])\n",
    "    lst_distance = sorted(lst_distance, key=lambda x: x[3], reverse=False)\n",
    "    lst_distance = lst_distance[:200]\n",
    "    df_distance = pd.DataFrame(lst_distance, columns=[\"id\", \"Name1\", \"Name2\", \"Distance\"])\n",
    "    df_distance.to_csv(\"./temp/distance.csv\", encoding=\"utf-8\")\n",
    "    # Next, in the CSV file generated above, create a new label column with 1/0 representing match/mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance - label and extract the reuslt\n",
    "\n",
    "with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eDict = json.load(f)\n",
    "df_distance = pd.read_csv(\"./temp/distance.csv\", encoding=\"utf-8\")\n",
    "distancePos, distanceNeg = [], []\n",
    "if \"label\" not in df_distance.columns:\n",
    "    print(\"Please label the CSV file generated above first!\")\n",
    "else:\n",
    "    for i in range(len(df_distance)):\n",
    "        id = df_distance.loc[i, \"id\"].split(\"-\")\n",
    "        id1, id2 = int(id[0][1:]), int(id[1][1:])\n",
    "        d1, d2 = data1[id1], data2[id2]\n",
    "        if d1[\"Name\"] != df_distance.loc[i, \"Name1\"] or d2[\"Name\"] != df_distance.loc[i, \"Name2\"]:\n",
    "            print(\"There was an error in the process of responding to the dataset! Please check\")\n",
    "        matched.add(d1[\"id\"]+d2[\"id\"])\n",
    "        if df_distance.loc[i, \"label\"] == 1:\n",
    "            distancePos.append(combineEnt(d1, d2, True))\n",
    "        else:\n",
    "            distanceNeg.append(combineEnt(d1, d2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [00:04<00:00, 253.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Road - Generate similar top 200\n",
    "\n",
    "if os.path.exists(\"temp/road.csv\"):\n",
    "    print(\"Top 200 file (Road) already exist!\")\n",
    "else:\n",
    "    with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        eDict = json.load(f)\n",
    "    lst_road = []\n",
    "    for i in trange(len(data1)):\n",
    "        d1 = data1[i]\n",
    "        for d2 in data2:\n",
    "            name1 = d1[\"Name\"]; name2 = d2[\"Name\"]\n",
    "            road1 = d1[\"RoadInfo\"]; road2 = d2[\"RoadInfo\"]\n",
    "            if road1 == \"\" or road2 == \"\": continue\n",
    "            if d1[\"id\"] + d2[\"id\"] in matched: continue\n",
    "            cossim = 1 - cosine(eDict[road1], eDict[road2])\n",
    "            lst_road.append([d1[\"id\"]+\"-\"+d2[\"id\"], name1, name2, road1, road2, cossim])\n",
    "    lst_road = sorted(lst_road, key=lambda x: x[5], reverse=True)\n",
    "    lst_road = lst_road[:100]\n",
    "    df_road = pd.DataFrame(lst_road, columns=[\"id\", \"Name1\", \"Name2\", \"Road1\", \"Road2\", \"Cos_sim_road\"])\n",
    "    df_road.to_csv(\"./temp/road.csv\", encoding=\"utf-8\")\n",
    "    # Next, in the CSV file generated above, create a new label column with 1/0 representing match/mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Road - label and extract the reuslt\n",
    "\n",
    "with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eDict = json.load(f)\n",
    "df_road = pd.read_csv(\"./temp/road.csv\", encoding=\"utf-8\")\n",
    "roadPos, roadNeg = [], []\n",
    "if \"label\" not in df_road.columns:\n",
    "    print(\"Please label the CSV file generated above first!\")\n",
    "else:\n",
    "    for i in range(len(df_road)):\n",
    "        id = df_road.loc[i, \"id\"].split(\"-\")\n",
    "        id1, id2 = int(id[0][1:]), int(id[1][1:])\n",
    "        d1, d2 = data1[id1], data2[id2]\n",
    "        if d1[\"Name\"] != df_road.loc[i, \"Name1\"] or d2[\"Name\"] != df_road.loc[i, \"Name2\"]:\n",
    "            print(\"There was an error in the process of responding to the dataset! Please check\")\n",
    "        matched.add(d1[\"id\"]+d2[\"id\"])\n",
    "        if df_road.loc[i, \"label\"] == 1:\n",
    "            roadPos.append(combineEnt(d1, d2, True))\n",
    "        else:\n",
    "            roadNeg.append(combineEnt(d1, d2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [00:00<00:00, 9290.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Room - Generate similar top 200\n",
    "\n",
    "if os.path.exists(\"temp/room.csv\"):\n",
    "    print(\"Top 200 file (Room) already exist!\")\n",
    "else:\n",
    "    with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        eDict = json.load(f)\n",
    "    lst_room = []\n",
    "    for i in trange(len(data1)):\n",
    "        d1 = data1[i]\n",
    "        for d2 in data2:\n",
    "            name1 = d1[\"Name\"]; name2 = d2[\"Name\"]\n",
    "            room1 = d1[\"RoomInfo\"]; room2 = d2[\"RoomInfo\"]\n",
    "            if room1 == \"\" or room2 == \"\": continue\n",
    "            if d1[\"id\"] + d2[\"id\"] in matched: continue\n",
    "            cossim = 1 - cosine(eDict[room1], eDict[room2])\n",
    "            lst_room.append([d1[\"id\"]+\"-\"+d2[\"id\"], name1, name2, room1, room2, cossim])\n",
    "    lst_room = sorted(lst_room, key=lambda x: x[5], reverse=True)\n",
    "    lst_room = lst_room[:20]\n",
    "    df_room = pd.DataFrame(lst_room, columns=[\"id\", \"Name1\", \"Name2\", \"Room1\", \"Room2\", \"Cos_sim_room\"])\n",
    "    df_room.to_csv(\"./temp/room.csv\", encoding=\"utf-8\")\n",
    "    # Next, in the CSV file generated above, create a new label column with 1/0 representing match/mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room - label and extract the reuslt\n",
    "\n",
    "with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eDict = json.load(f)\n",
    "df_room = pd.read_csv(\"./temp/room.csv\", encoding=\"utf-8\")\n",
    "roomPos, roomNeg = [], []\n",
    "if \"label\" not in df_room.columns:\n",
    "    print(\"Please label the CSV file generated above first!\"\")\n",
    "else:\n",
    "    for i in range(len(df_room)):\n",
    "        id = df_room.loc[i, \"id\"].split(\"-\")\n",
    "        id1, id2 = int(id[0][1:]), int(id[1][1:])\n",
    "        d1, d2 = data1[id1], data2[id2]\n",
    "        if d1[\"Name\"] != df_room.loc[i, \"Name1\"] or d2[\"Name\"] != df_room.loc[i, \"Name2\"]:\n",
    "            print(\"There was an error in the process of responding to the dataset! Please check\")\n",
    "        matched.add(d1[\"id\"]+d2[\"id\"])\n",
    "        if df_room.loc[i, \"label\"] == 1:\n",
    "            roomPos.append(combineEnt(d1, d2, True))\n",
    "        else:\n",
    "            roomNeg.append(combineEnt(d1, d2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 560\n"
     ]
    }
   ],
   "source": [
    "# Collect positive and negative data\n",
    "\n",
    "positive = namePos + distancePos + roadPos + roomPos\n",
    "negative = nameNeg + distanceNeg + roadNeg + roomNeg\n",
    "print(len(positive), len(negative))\n",
    "\n",
    "import random\n",
    "random.seed(1023)\n",
    "random.shuffle(positive)\n",
    "random.shuffle(negative)\n",
    "\n",
    "with open(\"../train_data/positive.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(positive, f, ensure_ascii=False, indent=2)\n",
    "with open(\"../train_data/negative.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(negative, f, ensure_ascii=False, indent=2)\n",
    "# print(len(namePos))\n",
    "# print(len(distancePos))\n",
    "# print(len(roadPos))\n",
    "# print(len(roomPos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e8f95b9510f32b9cde8cca11ac739448e236da4e8c1197f2e248ca6ab198849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
