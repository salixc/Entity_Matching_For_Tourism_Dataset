{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080 259\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers.util import cos_sim  \n",
    "from sentence_transformers import SentenceTransformer as SBert\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy import sqrt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import os\n",
    "# modelName = 'princeton-nlp/sup-simcse-bert-base-uncased'\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "dataName1 = \"Ctrip\"\n",
    "dataName2 = \"Mafengwo\"\n",
    "\n",
    "with open(\"../train_data/%s_addr.json\"%dataName1, \"r\", encoding=\"utf-8\") as f:\n",
    "    data1 = json.load(f)\n",
    "with open(\"../train_data/%s_addr.json\"%dataName2, \"r\", encoding=\"utf-8\") as f:\n",
    "    data2 = json.load(f)\n",
    "print(len(data1), len(data2))\n",
    "\n",
    "positive, negative = [], []\n",
    "matched = set()\n",
    "# ！注意：因为这个matched集合贯穿整个文件用来防止提取出重复的实体对，\n",
    "# 所以不能重复运行某单个代码块，如果要重复运行某个代码块的话，\n",
    "# 需要从第一个代码块（也就是现在所在的这个）依次都运行一遍，\n",
    "# 保证这个matched集合的正确性\n",
    "\n",
    "def combineEnt(d1, d2, label):\n",
    "    rnt = {\"id\": \"%s-%s\"%(d1[\"id\"], d2[\"id\"])}\n",
    "    rnt[\"label\"] = label\n",
    "    for attr in list(d1.keys())[1:]:\n",
    "        rnt[attr] = [d1[attr], d2[attr]]\n",
    "    return rnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [00:33<00:00, 40.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# 预先对所有出现的字符串进行词向量计算，以减少后续过程的时间复杂度\n",
    "if os.path.exists(\"temp/pre_dict.json\"):\n",
    "    print(\"预处理字典文件已经存在！\")\n",
    "else:\n",
    "    eDict = {}\n",
    "    cols = [\"Name\", \"Address\", \"District\", \"RoadInfo\", \"Poi\", \"RoomInfo\"]\n",
    "    data = data1 + data2\n",
    "    for i in trange(len(data)):\n",
    "        d = data[i]\n",
    "        for col in cols:\n",
    "            target = d[col]\n",
    "            if target == None or target == \"\": continue\n",
    "            if target not in eDict:\n",
    "                eDict[target] = model.encode(target).tolist()\n",
    "    with open(\"./temp/pre_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eDict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "名称Top200文件已经存在！\n"
     ]
    }
   ],
   "source": [
    "# 名称 - 生成相似top200\n",
    "\n",
    "if os.path.exists(\"temp/name.csv\"):\n",
    "    print(\"名称Top200文件已经存在！\")\n",
    "else:\n",
    "    with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        eDict = json.load(f)\n",
    "    lst_name = []\n",
    "    for i in trange(len(data1)):\n",
    "        d1 = data1[i]\n",
    "        for d2 in data2:\n",
    "            name1 = d1[\"Name\"]; name2 = d2[\"Name\"]\n",
    "            if name1 == \"\" or name2 == \"\": continue\n",
    "            cossim = 1 - cosine(eDict[name1], eDict[name2])\n",
    "            lst_name.append([d1[\"id\"]+\"-\"+d2[\"id\"], name1, name2, cossim])\n",
    "    lst_name = sorted(lst_name, key=lambda x: x[3], reverse=True)\n",
    "    lst_name = lst_name[:200]\n",
    "    df_name = pd.DataFrame(lst_name, columns=[\"id\", \"Name1\", \"Name2\", \"Cos_sim_name\"])\n",
    "    df_name.to_csv(\"./temp/name.csv\", encoding=\"utf-8\")\n",
    "    # 接下来在上面生成的csv文件里，新建一个label列，用1/0表示匹配/不匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名称 - 打标并提取结果\n",
    "\n",
    "with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eDict = json.load(f)\n",
    "df_name = pd.read_csv(\"./temp/name.csv\", encoding=\"utf-8\")\n",
    "namePos, nameNeg = [], []\n",
    "if \"label\" not in df_name.columns:\n",
    "    print(\"请先对上面生成的csv文件进行打标！\")\n",
    "else:\n",
    "    for i in range(len(df_name)):\n",
    "        id = df_name.loc[i, \"id\"].split(\"-\")\n",
    "        id1, id2 = int(id[0][1:]), int(id[1][1:])\n",
    "        d1, d2 = data1[id1], data2[id2]\n",
    "        if d1[\"Name\"] != df_name.loc[i, \"Name1\"] or d2[\"Name\"] != df_name.loc[i, \"Name2\"]:\n",
    "            print(\"对应回数据集的过程出现错误！请检查\")\n",
    "        matched.add(d1[\"id\"]+d2[\"id\"])\n",
    "        if df_name.loc[i, \"label\"] == 1:\n",
    "            namePos.append(combineEnt(d1, d2, True))\n",
    "        else:\n",
    "            nameNeg.append(combineEnt(d1, d2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "距离Top200文件已经存在！\n"
     ]
    }
   ],
   "source": [
    "# 距离 - 生成相似top200\n",
    "\n",
    "if os.path.exists(\"temp/distance.csv\"):\n",
    "    print(\"距离Top200文件已经存在！\")\n",
    "else:\n",
    "    from geopy.distance import geodesic\n",
    "    with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        eDict = json.load(f)\n",
    "    lst_distance = []\n",
    "    for i in trange(len(data1)):\n",
    "        d1 = data1[i]\n",
    "        for d2 in data2:\n",
    "            name1 = d1[\"Name\"]; name2 = d2[\"Name\"]\n",
    "            loc1 = d1[\"Location\"]; loc2 = d2[\"Location\"]\n",
    "            if loc1[0] is None or loc2[0] is None: continue\n",
    "            if d1[\"id\"] + d2[\"id\"] in matched: continue\n",
    "            distance = geodesic((loc1[1], loc1[0]), (loc2[1], loc2[0])).meters\n",
    "            if distance == 0: continue  # 坐标完全相同的可能是错误值\n",
    "            lst_distance.append([d1[\"id\"]+\"-\"+d2[\"id\"], name1, name2, distance])\n",
    "    lst_distance = sorted(lst_distance, key=lambda x: x[3], reverse=False)\n",
    "    lst_distance = lst_distance[:200]\n",
    "    df_distance = pd.DataFrame(lst_distance, columns=[\"id\", \"Name1\", \"Name2\", \"Distance\"])\n",
    "    df_distance.to_csv(\"./temp/distance.csv\", encoding=\"utf-8\")\n",
    "    # 接下来在上面生成的csv文件里，新建一个label列，用1/0表示匹配/不匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 距离 - 打标并提取结果\n",
    "\n",
    "with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eDict = json.load(f)\n",
    "df_distance = pd.read_csv(\"./temp/distance.csv\", encoding=\"utf-8\")\n",
    "distancePos, distanceNeg = [], []\n",
    "if \"label\" not in df_distance.columns:\n",
    "    print(\"请先对上面生成的csv文件进行打标！\")\n",
    "else:\n",
    "    for i in range(len(df_distance)):\n",
    "        id = df_distance.loc[i, \"id\"].split(\"-\")\n",
    "        id1, id2 = int(id[0][1:]), int(id[1][1:])\n",
    "        d1, d2 = data1[id1], data2[id2]\n",
    "        if d1[\"Name\"] != df_distance.loc[i, \"Name1\"] or d2[\"Name\"] != df_distance.loc[i, \"Name2\"]:\n",
    "            print(\"对应回数据集的过程出现错误！请检查\")\n",
    "        matched.add(d1[\"id\"]+d2[\"id\"])\n",
    "        if df_distance.loc[i, \"label\"] == 1:\n",
    "            distancePos.append(combineEnt(d1, d2, True))\n",
    "        else:\n",
    "            distanceNeg.append(combineEnt(d1, d2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [00:04<00:00, 253.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# 道路 - 生成相似top100\n",
    "\n",
    "if os.path.exists(\"temp/road.csv\"):\n",
    "    print(\"道路Top100文件已经存在！\")\n",
    "else:\n",
    "    with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        eDict = json.load(f)\n",
    "    lst_road = []\n",
    "    for i in trange(len(data1)):\n",
    "        d1 = data1[i]\n",
    "        for d2 in data2:\n",
    "            name1 = d1[\"Name\"]; name2 = d2[\"Name\"]\n",
    "            road1 = d1[\"RoadInfo\"]; road2 = d2[\"RoadInfo\"]\n",
    "            if road1 == \"\" or road2 == \"\": continue\n",
    "            if d1[\"id\"] + d2[\"id\"] in matched: continue\n",
    "            cossim = 1 - cosine(eDict[road1], eDict[road2])\n",
    "            lst_road.append([d1[\"id\"]+\"-\"+d2[\"id\"], name1, name2, road1, road2, cossim])\n",
    "    lst_road = sorted(lst_road, key=lambda x: x[5], reverse=True)\n",
    "    lst_road = lst_road[:100]\n",
    "    df_road = pd.DataFrame(lst_road, columns=[\"id\", \"Name1\", \"Name2\", \"Road1\", \"Road2\", \"Cos_sim_road\"])\n",
    "    df_road.to_csv(\"./temp/road.csv\", encoding=\"utf-8\")\n",
    "    # 接下来在上面生成的csv文件里，新建一个label列，用1/0表示匹配/不匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 道路 - 打标并提取结果\n",
    "\n",
    "with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eDict = json.load(f)\n",
    "df_road = pd.read_csv(\"./temp/road.csv\", encoding=\"utf-8\")\n",
    "roadPos, roadNeg = [], []\n",
    "if \"label\" not in df_road.columns:\n",
    "    print(\"请先对上面生成的csv文件进行打标！\")\n",
    "else:\n",
    "    for i in range(len(df_road)):\n",
    "        id = df_road.loc[i, \"id\"].split(\"-\")\n",
    "        id1, id2 = int(id[0][1:]), int(id[1][1:])\n",
    "        d1, d2 = data1[id1], data2[id2]\n",
    "        if d1[\"Name\"] != df_road.loc[i, \"Name1\"] or d2[\"Name\"] != df_road.loc[i, \"Name2\"]:\n",
    "            print(\"对应回数据集的过程出现错误！请检查\")\n",
    "        matched.add(d1[\"id\"]+d2[\"id\"])\n",
    "        if df_road.loc[i, \"label\"] == 1:\n",
    "            roadPos.append(combineEnt(d1, d2, True))\n",
    "        else:\n",
    "            roadNeg.append(combineEnt(d1, d2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [00:00<00:00, 9290.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 房间 - 生成相似top20\n",
    "\n",
    "if os.path.exists(\"temp/room.csv\"):\n",
    "    print(\"房间Top20文件已经存在！\")\n",
    "else:\n",
    "    with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        eDict = json.load(f)\n",
    "    lst_room = []\n",
    "    for i in trange(len(data1)):\n",
    "        d1 = data1[i]\n",
    "        for d2 in data2:\n",
    "            name1 = d1[\"Name\"]; name2 = d2[\"Name\"]\n",
    "            room1 = d1[\"RoomInfo\"]; room2 = d2[\"RoomInfo\"]\n",
    "            if room1 == \"\" or room2 == \"\": continue\n",
    "            if d1[\"id\"] + d2[\"id\"] in matched: continue\n",
    "            cossim = 1 - cosine(eDict[room1], eDict[room2])\n",
    "            lst_room.append([d1[\"id\"]+\"-\"+d2[\"id\"], name1, name2, room1, room2, cossim])\n",
    "    lst_room = sorted(lst_room, key=lambda x: x[5], reverse=True)\n",
    "    lst_room = lst_room[:20]\n",
    "    df_room = pd.DataFrame(lst_room, columns=[\"id\", \"Name1\", \"Name2\", \"Room1\", \"Room2\", \"Cos_sim_room\"])\n",
    "    df_room.to_csv(\"./temp/room.csv\", encoding=\"utf-8\")\n",
    "    # 接下来在上面生成的csv文件里，新建一个label列，用1/0表示匹配/不匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 房间 - 打标并提取结果\n",
    "\n",
    "with open(\"./temp/pre_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eDict = json.load(f)\n",
    "df_room = pd.read_csv(\"./temp/room.csv\", encoding=\"utf-8\")\n",
    "roomPos, roomNeg = [], []\n",
    "if \"label\" not in df_room.columns:\n",
    "    print(\"请先对上面生成的csv文件进行打标！\")\n",
    "else:\n",
    "    for i in range(len(df_room)):\n",
    "        id = df_room.loc[i, \"id\"].split(\"-\")\n",
    "        id1, id2 = int(id[0][1:]), int(id[1][1:])\n",
    "        d1, d2 = data1[id1], data2[id2]\n",
    "        if d1[\"Name\"] != df_room.loc[i, \"Name1\"] or d2[\"Name\"] != df_room.loc[i, \"Name2\"]:\n",
    "            print(\"对应回数据集的过程出现错误！请检查\")\n",
    "        matched.add(d1[\"id\"]+d2[\"id\"])\n",
    "        if df_room.loc[i, \"label\"] == 1:\n",
    "            roomPos.append(combineEnt(d1, d2, True))\n",
    "        else:\n",
    "            roomNeg.append(combineEnt(d1, d2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 560\n"
     ]
    }
   ],
   "source": [
    "# 收集所有正负标记数据\n",
    "\n",
    "positive = namePos + distancePos + roadPos + roomPos\n",
    "negative = nameNeg + distanceNeg + roadNeg + roomNeg\n",
    "print(len(positive), len(negative))\n",
    "\n",
    "import random\n",
    "random.seed(1023)\n",
    "random.shuffle(positive)\n",
    "random.shuffle(negative)\n",
    "\n",
    "with open(\"../train_data/positive.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(positive, f, ensure_ascii=False, indent=2)\n",
    "with open(\"../train_data/negative.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(negative, f, ensure_ascii=False, indent=2)\n",
    "# print(len(namePos))\n",
    "# print(len(distancePos))\n",
    "# print(len(roadPos))\n",
    "# print(len(roomPos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e8f95b9510f32b9cde8cca11ac739448e236da4e8c1197f2e248ca6ab198849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
